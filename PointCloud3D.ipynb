{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugenioBugli/3DPointCloud/blob/main/PointCloud3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_0FERiAhMW8"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kBbL55fBHVY2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install open3d\n",
        "# !pip install torchinfo\n",
        "!pip install torch_scatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "su0uRyWQE_On",
        "outputId": "005e7da5-0246-4905-ee3a-4f4e8d808e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.4.1+cu121  Device: cpu\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "# from torchinfo import summary\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import DatasetFolder\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_scatter import scatter_mean\n",
        "\n",
        "import open3d as o3d\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/CV/MPI-FAUST\"\n",
        "training_path = drive_path + \"/training\"\n",
        "test_path = drive_path + \"/test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFBi0FgT9WKe"
      },
      "source": [
        "# 1] Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OvjZrNG19WKe"
      },
      "outputs": [],
      "source": [
        "class FAUST_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "        This class is used to load the FAUST dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path, sampling_type=\"RANDOM\", sampling_size=1024, partition=\"TRAIN\", transform=None):\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.SamplingType = sampling_type # either RANDOM or IMPORTANCE\n",
        "        self.SamplingSize = sampling_size\n",
        "        self.transform = transform\n",
        "\n",
        "        self.dataset = DatasetFolder(\n",
        "            root = self.data_path,\n",
        "            loader = o3d.io.read_point_cloud,\n",
        "            extensions = ('ply',),\n",
        "            allow_empty = True,\n",
        "        )\n",
        "\n",
        "        # this is needed since we have access to the registrations only for the training set\n",
        "        # target : this number is used to identify the subdirectory in DatasetFolder\n",
        "        # id : this numbers is used to identify the cloud, since I have use different containers to store them:\n",
        "        #   id = 0 : rawClouds (after sampling they all have the same shape)\n",
        "        #   id = 1 : groundTruths (different shapes)\n",
        "        if partition == \"TRAIN\":\n",
        "            self.rawClouds = self.ExtractClouds(target=2, id=0)\n",
        "            self.groundTruths = self.ExtractClouds(target=1, id=1)\n",
        "        else :\n",
        "            self.rawClouds = self.ExtractClouds(target=1, id=0)\n",
        "            self.groundTruths = self.ExtractClouds(target=1, id=1)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.transform:\n",
        "            return self.transform(self.rawClouds[index]).to(torch.float32).squeeze(0), self.transform(self.groundTruths[index]).to(torch.float32).squeeze(0)\n",
        "\n",
        "        return self.rawClouds[index], self.groundTruths[index]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rawClouds)\n",
        "\n",
        "    def ExtractClouds(self, target, id):\n",
        "        # Sort the file imported from the dataset (needed to match scans and registrations)\n",
        "        unsorted_file_clouds = [sample for sample, t in self.dataset.samples if t == target]\n",
        "        file_clouds = sorted(unsorted_file_clouds, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "        # the initialization is different since the ground truths have different number of points !\n",
        "        if id == 0:\n",
        "            clouds = []\n",
        "            for i in range(len(file_clouds)):\n",
        "                c = o3d.io.read_point_cloud(file_clouds[i])\n",
        "                clouds.append(self.SamplingFunction(c))\n",
        "        else:\n",
        "            clouds = dict()\n",
        "            for i in range(len(file_clouds)):\n",
        "                clouds[i] = np.asarray(o3d.io.read_point_cloud(file_clouds[i]).points)\n",
        "\n",
        "        return clouds\n",
        "\n",
        "    def plotCloud(self, cloud):\n",
        "        \"\"\"\n",
        "        This function is used to plot the Point Cloud\n",
        "\n",
        "        Args:\n",
        "            cloud (np.array): Point Cloud\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if len(cloud.shape) == 3:\n",
        "            # when using DataLoader you have a shape (1, SAMPLING_SIZE, 3)\n",
        "            cloud = cloud[0]\n",
        "\n",
        "        fig = go.Figure(\n",
        "            data=[\n",
        "            go.Scatter3d(\n",
        "                x =cloud[:,0], y=cloud[:,1], z=cloud[:,2],\n",
        "                mode = 'markers',\n",
        "                marker = dict(size=0.5, color=[])\n",
        "            )\n",
        "        ],\n",
        "        layout=dict(\n",
        "            scene=dict(\n",
        "                xaxis=dict(visible=True),\n",
        "                yaxis=dict(visible=True),\n",
        "                zaxis=dict(visible=True),\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def SamplingFunction(self, cloud):\n",
        "        \"\"\"\n",
        "            This function is used to sample a small Subset of points from the Point Clouds inside our Dataset.\n",
        "\n",
        "            @INPUT :\n",
        "                > cloud : Point Cloud extracted from .ply file\n",
        "\n",
        "            @OUTPUT :\n",
        "                > sampled_cloud : Sampled Point Cloud\n",
        "        \"\"\"\n",
        "\n",
        "        if self.SamplingType == 'RANDOM':\n",
        "            points = np.asarray(cloud.points)\n",
        "            indices = np.random.choice(len(points), size=self.SamplingSize)\n",
        "            sampled_cloud = points[indices]\n",
        "        if self.SamplingType == 'IMPORTANCE':\n",
        "            numOfNeighbors = 20\n",
        "            # estimate normal vectors to the surface at each point of the cloud\n",
        "            cloud.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamKNN(knn=numOfNeighbors))\n",
        "            tree = o3d.geometry.KDTreeFlann(cloud) # faster\n",
        "            # loop over the points and compute curvature\n",
        "            curvature = np.zeros(len(cloud.points))\n",
        "            for i in range(len(cloud.points)):\n",
        "                # find indices of the neighbors\n",
        "                [_ , idx, _] = tree.search_knn_vector_3d(cloud.points[i], numOfNeighbors)\n",
        "                neighbors = np.asarray(cloud.points)[idx, :]\n",
        "                # compute covariance matrix for each point\n",
        "                covarianceMat = np.cov(neighbors.T)\n",
        "                # extract eigenvalues\n",
        "                eigen, _ = np.linalg.eigh(covarianceMat)\n",
        "                # compute curvature\n",
        "                curvature[i] = min(eigen) / sum(eigen)\n",
        "            # extract the best SamplingPoints points\n",
        "            maxCurvaturePoints = curvature.argsort()[-self.SamplingSize:]\n",
        "            sampled_cloud = np.asarray(cloud.points)[maxCurvaturePoints]\n",
        "\n",
        "        return sampled_cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T676hUtbMzcD"
      },
      "outputs": [],
      "source": [
        "# Save your preprocessed Dataset:\n",
        "def SaveDataset(dataset, path):\n",
        "    torch.save(dataset, \"/content/drive/MyDrive/CV/PreProcessed/\"+path)\n",
        "\n",
        "# Load your preprocessed Dataset:\n",
        "def LoadDataset(path):\n",
        "    return torch.load(\"/content/drive/MyDrive/CV/PreProcessed/\"+path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Save Dataset\n"
      ],
      "metadata": {
        "id": "ibjyXRMqN7eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainSetRandom = FAUST_Dataset(training_path, sampling_type=\"RANDOM\", sampling_size=1024, partition=\"TRAIN\", transform=transforms.ToTensor())\n",
        "#SaveDataset(trainSetRandom, \"trainSet_random.pt\")\n",
        "print(\"Random Sampling saved! \\n\")\n",
        "#trainSetCurvature = FAUST_Dataset(training_path, sampling_type=\"IMPORTANCE\", sampling_size=1024, partition=\"TRAIN\", transform=transforms.ToTensor())\n",
        "#SaveDataset(trainSetCurvature, \"trainSet_curvature.pt\")\n",
        "#print(\"Curvature-based Sampling saved! \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL7v8ly-OE4F",
        "outputId": "e1e02140-c7cb-494e-90fe-9b75134b882b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Sampling saved! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh8ugKUp9WKf"
      },
      "source": [
        "# 2] Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abAwINI9WKf"
      },
      "source": [
        "## 2.1) ResBlock + Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OZfg7nQw9WKf"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define a Residual Block, which is one of the main component of the ResNetPointNet architecture\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=64, n_points=1024, h_dim=32, out_dim=64):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_points = n_points\n",
        "\n",
        "        #> First part of the Block\n",
        "\n",
        "        self.fc1 = nn.Linear(\n",
        "            in_dim,\n",
        "            h_dim\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(self.n_points)\n",
        "\n",
        "        #> Second part of the Block\n",
        "\n",
        "        self.fc2 = nn.Linear(\n",
        "            h_dim,\n",
        "            out_dim\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(self.n_points)\n",
        "\n",
        "        #> Skip connection\n",
        "\n",
        "        if in_dim != out_dim:\n",
        "            # size mismatch (never happen in my case)\n",
        "            self.residual = nn.Linear(in_dim, out_dim)\n",
        "        else:\n",
        "            # same size\n",
        "            self.residual = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (b,p,in_dim) = (b,p,64)\n",
        "\n",
        "        first_part = F.relu(self.bn1(self.fc1(x))) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        second_part = self.bn2(self.fc2(first_part)) # (b,p,32) -> (b,p,64)\n",
        "\n",
        "        if self.residual is None:\n",
        "            # no size mismatch\n",
        "            res = x # (b,p,64)\n",
        "        else:\n",
        "            # transformation if there is a size mismatch in_dim != out_dim\n",
        "            res = self.residual(x) # (b,p,in_dim) -> (b,p,64)\n",
        "\n",
        "        # add residual connection\n",
        "        third_part = second_part + res # (b,p,64) -> (b,p,64)\n",
        "\n",
        "        return F.relu(third_part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "443lgCMp9WKf"
      },
      "outputs": [],
      "source": [
        "def CoordinateNormalization(input_cloud, planeType='xz'):\n",
        "    \"\"\"\n",
        "        This Function is used to Normalize coordinates from the input point cloud.\n",
        "        After this operation the cloud will be centered at the origin and all coordinates will be inside a standard range [0,1].\n",
        "        The results will be the normalized point cloud focused only on the plane coordinate. This means that we will have only 2 coordinates for\n",
        "        our points instead of 3.\n",
        "\n",
        "        Args:\n",
        "            input_cloud : (batch_size, n_points, 3) tensor\n",
        "            planeType: (str) represent the kind of plane that we are using (xz, xy, yz)\n",
        "\n",
        "        Returns:\n",
        "            norm_cloud : (batch_size, n_points, 2) tensor\n",
        "    \"\"\"\n",
        "    # Extract coordinates according to the planeType you are using\n",
        "    if planeType == 'xz':\n",
        "        plane = input_cloud[:, :, [0,2]] # (b,p,3) -> (b,p,2)\n",
        "    elif planeType == 'xy':\n",
        "        plane = input_cloud[:, :, [0,1]] # (b,p,3) -> (b,p,2)\n",
        "    elif planeType == 'yz':\n",
        "        plane = input_cloud[:, :, [1,2]] # (b,p,3) -> (b,p,2)\n",
        "\n",
        "\n",
        "    norm_cloud = (plane - plane.min())/(plane.max() - plane.min() + 10e-6)\n",
        "\n",
        "    return norm_cloud\n",
        "\n",
        "\n",
        "def PlaneCoordinate2Index(input_cloud, voxel_size=1.0/64):\n",
        "    \"\"\"\n",
        "        This function is used to Obtain an Index from the Plane Coordinates of the original input cloud.\n",
        "        We need to transform Coordinates into Indices because we would like to discretize space into Voxels.\n",
        "        This index is extremely important since is used to perform Local Pooling, one of the operation of the ResNetPointNet Architecture.\n",
        "\n",
        "        Args:\n",
        "            input_cloud : (batch_size, n_points, 2) tensor\n",
        "            voxel_size : (float) represent how large the Voxels (Local Regions) are (default = 0.2)\n",
        "\n",
        "        Returns:\n",
        "            index : (batch_size, n_points, 2) tensor\n",
        "    \"\"\"\n",
        "    return torch.floor(input_cloud/voxel_size).long()\n",
        "\n",
        "\n",
        "def LocalPooling(norm_cloud, indices, input_cloud):\n",
        "    \"\"\"\n",
        "        Unlike the Vanilla PointNet, we perform Local Max Pooling on the output of each ResBlock and then we concatenate the result\n",
        "        with the features before the operation.\n",
        "        In order to perform this operation we have to normalize the planes coordinates and transform into indices.\n",
        "        These indices represent which voxel each point belongs to.\n",
        "\n",
        "        ResBlock ________________\n",
        "            |                    |\n",
        "            |                    |\n",
        "        LocalPool                |\n",
        "            |                    |\n",
        "            v                    |\n",
        "            +   <----------------'\n",
        "\n",
        "        Args:\n",
        "            norm_cloud: (batch_size, num_points, 2) tensor that represent the normalized input cloud related to a specific plane (xz)\n",
        "            indices: (batch_size, num_points, 2) tensor that represent the indices of the input cloud related to the Voxel\n",
        "            input_cloud: (batch_size, num_points, n_features) tensor that represent the input cloud\n",
        "\n",
        "        Returns:\n",
        "            pool_cloud: tensor of shape (batch_size, num_points, n_features)\n",
        "    \"\"\"\n",
        "\n",
        "    pool_cloud = torch.zeros_like(input_cloud)\n",
        "\n",
        "    for b in range (norm_cloud.shape[0]):\n",
        "        buckets = dict()\n",
        "\n",
        "        # each point in the batch will have its own index along with the ones given by the PlaneCoordinate2Index transformation\n",
        "        for i, voxel_idx in enumerate(indices[b]):\n",
        "            # each point is identified by its own key (index)\n",
        "\n",
        "            voxel_key = tuple(voxel_idx.tolist())\n",
        "            if voxel_key not in buckets:\n",
        "                buckets[voxel_key] = []\n",
        "\n",
        "            # insert inside the correct bucket the point\n",
        "            buckets[voxel_key].append(input_cloud[b][i])\n",
        "\n",
        "\n",
        "        for voxel_key, points_list in buckets.items():\n",
        "            # inside each buckets you have a list of points (tensors)\n",
        "\n",
        "            if len(points_list) > 0:\n",
        "                # extract the local maximum\n",
        "                bucket_points = torch.stack(points_list)\n",
        "                max_p = torch.max(bucket_points, dim=0)[0]\n",
        "\n",
        "                # assign the maximum to all the points inside the buckets\n",
        "\n",
        "                for i, voxel_idx in enumerate(indices[b]):\n",
        "                    if torch.equal(voxel_idx, torch.Tensor(voxel_key)):\n",
        "                        pool_cloud[b][i] = max_p\n",
        "    return pool_cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk_D19aI9WKg"
      },
      "source": [
        "## 2.2) ResNetPointNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qYhVqWYM9WKg"
      },
      "outputs": [],
      "source": [
        "class ResNetPointNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the PointNet model used to form a feature embedding for each point in the Point Cloud given in input.\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3)\n",
        "\n",
        "            > Fully Connected Layer (3, in_dim=64)\n",
        "            > 5 Residual Blocks with Local Pooling and Concatenation\n",
        "            > Fully Connected Layer (out_dim=32, out_dim=32)\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=64, n_points=1024, res_dim=32, out_dim=32, n_blocks=5):\n",
        "        super(ResNetPointNet, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(3, in_dim)\n",
        "\n",
        "        self.res = nn.ModuleList([\n",
        "            ResBlock(in_dim, n_points, res_dim, out_dim) for n_res in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (b,p,3)\n",
        "\n",
        "        # Extract Normalized coordinates and indices to perform local pooling\n",
        "\n",
        "        norm_coord = CoordinateNormalization(x) # (b,p,2)\n",
        "        coord_indices = PlaneCoordinate2Index(norm_coord) # (b,p,2)\n",
        "\n",
        "        # First FC Layer\n",
        "        fc1 = F.relu(self.fc1(x)) # (b,p,3) -> (b,p,64)\n",
        "\n",
        "        # First ResBlock\n",
        "        res = self.res[0](fc1) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        # 2-5 ResBlock\n",
        "        for res_block in self.res[1:]:\n",
        "            # Local Pooling\n",
        "            pool = LocalPooling(norm_coord, coord_indices, res) # (b,p,32)\n",
        "            # Concatenation\n",
        "            concat = torch.cat([res, pool], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "            # following residual block\n",
        "            res = res_block(concat) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        # Last FC Layer\n",
        "        final = F.relu(self.fc2(res)) # (b,p,32) -> (b,p,32)\n",
        "\n",
        "        print(f\"ResNetPointNet : input ({x.shape}) ---> output ({final.shape}) \\n\")\n",
        "\n",
        "        return final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHoALeYs9WKg"
      },
      "source": [
        "## 2.3) Plane Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ASem9l-E9WKg"
      },
      "outputs": [],
      "source": [
        "class SimplePointNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define a simple variant of the PointNet Model, which is one of the main components of the Plane Predictor Network.\n",
        "        This Network will provide us a global context of the Input Point Clouds\n",
        "        Architecture Design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3) which represent the Input Point Clouds\n",
        "\n",
        "            > Fully Connected Layer (3, 64)\n",
        "\n",
        "              |> Fully Connected Layer (64, 32)\n",
        "            2*|> Global Max Pooling\n",
        "              |> Concatenation btw Pooled and unpooled features\n",
        "\n",
        "            > Fully Connected Layer (64, 32)\n",
        "            > Global Max Pooling\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32) which will be used by the rest of the Plane Predictor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, in_dim=64, n_points=1024, hid_dim=32, out_dim=64):\n",
        "        super(SimplePointNet, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_points = n_points\n",
        "\n",
        "        self.initial_fc = nn.Linear(in_features=3, out_features=in_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.final_fc = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.initial_fc(x) # (b,p,3) --> (b,p,64)\n",
        "\n",
        "        x1 = self.fc1(x) # (b,p,64) --> (b,p,32)\n",
        "        x1_t = x1.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        pool_x1 = self.pool(x1_t) # (b,32,p) -> (b,32,1)\n",
        "        exp_pool_x1 = pool_x1.transpose(1,2).expand(self.batch_size, self.n_points, 32) # (b,32,1) -> (b,1,32) -> (b,p,32)\n",
        "        concat_x1 = torch.cat([x1, exp_pool_x1], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "\n",
        "        x2 = self.fc2(concat_x1) # (b,p,64) -> (b,p,32)\n",
        "        x2_t = x2.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        pool_x2 = self.pool(x2_t) # (b,32,p) -> (b,32,1)\n",
        "        exp_pool_x2 = pool_x2.transpose(1,2).expand(self.batch_size, self.n_points, 32) # (b,32,1) -> (b,1,32) -> (b,p,32)\n",
        "        concat_x2 = torch.cat([x2, exp_pool_x2], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "\n",
        "        pre_pool_out = self.final_fc(concat_x2) # (b,p,64) -> (b,p,32)\n",
        "        pre_pool_out_t = pre_pool_out.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        out = self.pool(pre_pool_out_t) # (b,32,p) -> (b,32,1)\n",
        "\n",
        "        print(f\"SimplePointNet : input ({x.shape}) ---> output ({out.transpose(1,2).shape}) \\n\")\n",
        "        return out.transpose(1,2) # (b,32,1) -> (b,1,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6cWO2Alj9WKg"
      },
      "outputs": [],
      "source": [
        "class PlanePredictor(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the Plane Predictor of our Architecture, which will predict the plane parameters of L dynamic planes\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3) which represent Point Clouds\n",
        "\n",
        "            > Simple PointNet which learns the global context of the input point clouds\n",
        "            > This information is encoded into one global feature by using Max Pooling\n",
        "            > 4 Fully Connected Layers with hidden dimension = 32\n",
        "            > L Shallow Networks with hidden dimension = 3 which will give us the Predicted Plane Parameters\n",
        "            > L Fully Connected Layers with 1 layer and hidden dimension = D (same as point cloud encoder hidden dimension)\n",
        "            > Each plane-specific feature is expanded to N x D to match the output of the point cloud encoder, which will be summed together\n",
        "\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32) which will be processed into U-Net\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=32, n_points=1024, n_fc=4, L=4):\n",
        "        super(PlanePredictor, self).__init__()\n",
        "\n",
        "        self.pointNet = SimplePointNet(batch_size=1)\n",
        "        self.n_points = n_points\n",
        "\n",
        "        # 4 FC layers with hidden dim = 32\n",
        "\n",
        "        self.four_fc = nn.ModuleList(\n",
        "            [nn.Linear(in_dim, in_dim) for i in range(n_fc)]\n",
        "        )\n",
        "\n",
        "        # Plane parameters (L shallow networks with hidden dim = 3)\n",
        "\n",
        "        self.first_shallow = nn.Linear(in_dim, 3)\n",
        "        self.shallows = nn.ModuleList(\n",
        "            [nn.Linear(3, 3) for i in range(L-1)]\n",
        "        )\n",
        "\n",
        "        # L FC layers with hidden dim = 32\n",
        "\n",
        "        self.first_fc = nn.Linear(3, in_dim)\n",
        "        self.L_fc = nn.ModuleList(\n",
        "            [nn.Linear(in_dim, in_dim) for i in range(L-1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        flc = self.pointNet(x) # (b,p,3) -> (b,1,32)\n",
        "\n",
        "        # 4 FC layers with hidden dim = 32\n",
        "\n",
        "        for fc in self.four_fc:\n",
        "            flc = F.relu(fc(flc))  # (b,1,32) -> (b,1,32)\n",
        "\n",
        "        # Plane parameters ( L Shallow Networks )\n",
        "\n",
        "        first_sh = F.relu(self.first_shallow(flc)) # (b,1,32) -> (b,1,3)\n",
        "        shal = first_sh\n",
        "        for s in self.shallows:\n",
        "            shal = F.relu(s(shal)) # (b,1,3) -> (b,1,3)\n",
        "\n",
        "        # L FC networks dim = 32\n",
        "\n",
        "        first_fc_L = F.relu(self.first_fc(shal)) # (b,1,3) -> (b,1,32)\n",
        "        L_fully = first_fc_L\n",
        "        for fc in self.L_fc:\n",
        "            L_fully = F.relu(fc(L_fully))  # (b,1,32) -> (b,1,32)\n",
        "\n",
        "        # Expansion\n",
        "\n",
        "        out = L_fully.expand(x.shape[0], self.n_points, first_fc_L.shape[-1]) # (b,1,32) -> (b,p,32)\n",
        "\n",
        "        print(f\"PlanePredictor : input ({x.shape}) ---> output ({out.shape}) \\n\")\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XbmuG1FR9WKh"
      },
      "outputs": [],
      "source": [
        "def FeatureProjection(input_cloud, features, resolution=64):\n",
        "    \"\"\"\n",
        "        This function is used to extract the Feauture Projections from the output of the Plane Predictor summed to the output of the ResNetPointNet\n",
        "        Args:\n",
        "            input_cloud: (batch_size, num_points, 3) tensor that represent the input cloud\n",
        "            features: (batch_size), num_points, n_features=32) tensor that represent the features extracted by the Plane Predictor summed to the ones of ResNetPointNet\n",
        "\n",
        "        Returns:\n",
        "            proj_features: (batch_size, num_points, ) tensor that represent the projected cloud\n",
        "    \"\"\"\n",
        "    norm_cloud = CoordinateNormalization(input_cloud) # (b,p,3) -> (b,p,2)\n",
        "    indices = PlaneCoordinate2Index(norm_cloud) # (b,p,2) -> (b,p,2)\n",
        "    flatten_indices = indices[:, :, 0] + indices[:, :, 1]*resolution # (b,p,2) -> (b,p)\n",
        "\n",
        "    proj_features = torch.zeros(input_cloud.shape[0], features.shape[2], resolution**2) # (b,32,res^2) = (b,32,4096)\n",
        "\n",
        "    proj_features = scatter_mean(features.transpose(2,1), flatten_indices.unsqueeze(1), dim=2, out=proj_features)\n",
        "\n",
        "    print(f\"Feature Projection : input ({input_cloud.shape}) ---> output ({proj_features.reshape(input_cloud.shape[0], features.shape[2], resolution, resolution).shape}) \\n\")\n",
        "\n",
        "    return proj_features.reshape(input_cloud.shape[0], features.shape[2], resolution, resolution) # (b,32,res,res) = (b,32,64,64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BEqvWJV9WKh"
      },
      "source": [
        "## 2.4) UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eVGhQD969WKh"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the UNet of our Architecture, which is the final part of our Encoder.\n",
        "        Architecture design:\n",
        "\n",
        "        @ INPUT: Tensor of shape (batch_size, 32, 64, 64)\n",
        "            # Encoder:\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > MaxPool2D\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > MaxPool2D\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > MaxPool2D\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "            # BottleNeck:\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "            # Decoder:\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > Conv2D\n",
        "\n",
        "        @ OUTPUT: Tensor of shape (batch_size, num_points, 32)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=32, out_dim=32, features_dim=64, n_points=1024):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "\n",
        "        ## Block 1\n",
        "\n",
        "        self.e_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_dim,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_1 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        ## Block 2\n",
        "\n",
        "        self.e_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_2 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        ## Block 3\n",
        "\n",
        "        self.e_3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_3 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        ## Block 4\n",
        "\n",
        "        self.e_4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_4 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "\n",
        "        self.b = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*16,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*16,\n",
        "                out_channels=features_dim*16,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*16),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "\n",
        "        ## Block 1\n",
        "\n",
        "        self.d_upconv1 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*16,\n",
        "            out_channels=features_dim*8,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*16,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        ## Block 2\n",
        "\n",
        "        self.d_upconv2 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*8,\n",
        "            out_channels=features_dim*4,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        ## Block 3\n",
        "\n",
        "        self.d_upconv3 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*4,\n",
        "            out_channels=features_dim*2,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        ## Block 4\n",
        "\n",
        "        self.d_upconv4 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*2,\n",
        "            out_channels=features_dim,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=out_dim,\n",
        "                kernel_size=1\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        enc1 = self.e_1(x)\n",
        "        #print(f\"Encoder block 1 : input ({x.shape}) ---> output ({enc1.shape})\")\n",
        "        enc2 = self.e_2(self.p_1(enc1))\n",
        "        #print(f\"Encoder block 2 : input ({self.p_1(enc1).shape}) ---> output ({enc2.shape})\")\n",
        "        enc3 = self.e_3(self.p_2(enc2))\n",
        "        #print(f\"Encoder block 3 : input ({self.p_2(enc2).shape}) ---> output ({enc3.shape})\")\n",
        "        enc4 = self.e_4(self.p_3(enc3))\n",
        "        #print(f\"Encoder block 4 : input ({self.p_3(enc3).shape}) ---> output ({enc4.shape})\")\n",
        "\n",
        "        bottle = self.b(self.p_4(enc4))\n",
        "        #print(f\"BottleNeck : input ({self.p_4(enc4).shape}) ---> output ({bottle.shape})\")\n",
        "\n",
        "        dec1 = self.d_upconv1(bottle)\n",
        "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
        "        dec1 = self.d_1(dec1)\n",
        "        #print(f\"Decoder block 1 : input ({bottle.shape}) ---> output ({dec1.shape})\")\n",
        "        dec2 = self.d_upconv2(dec1)\n",
        "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
        "        dec2 = self.d_2(dec2)\n",
        "        #print(f\"Decoder block 2 : input ({dec1.shape}) ---> output ({dec2.shape})\")\n",
        "        dec3 = self.d_upconv3(dec2)\n",
        "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
        "        dec3 = self.d_3(dec3)\n",
        "        #print(f\"Decoder block 3 : input ({dec2.shape}) ---> output ({dec3.shape})\")\n",
        "        dec4 = self.d_upconv4(dec3)\n",
        "        dec4 = torch.cat((dec4, enc1), dim=1)\n",
        "        dec4 = self.d_4(dec4)\n",
        "        #print(f\"Decoder block 4 : input ({dec3.shape}) ---> output ({dec4.shape})\")\n",
        "        out = self.final(dec4)\n",
        "        #print(f\"Final : input ({dec4.shape}) ---> output ({out.shape})\")\n",
        "\n",
        "\n",
        "        print(f\"UNet : input ({x.shape}) ---> output ({out.shape}) \\n\")\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fME3ek-b9WKh"
      },
      "source": [
        "## 2.5) Complete Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wEnfYEQZ9WKh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=3, out_dim=32, n_points=1024, n_blocks=5):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.resnet_pointnet = ResNetPointNet()\n",
        "        self.plane_predictor = PlanePredictor()\n",
        "        self.UNet = UNet()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        resnet = self.resnet_pointnet(x)\n",
        "        plane_pred = self.plane_predictor(x)\n",
        "        # here we need to perform features projection\n",
        "        features_proj = FeatureProjection(x, resnet + plane_pred)\n",
        "        out = self.UNet(features_proj)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xfwdl8S9WKi"
      },
      "source": [
        "# 3] Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W3Cu6WzB9WKi"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the Decoder Network. The architecture is composed by 5 ResNet blocks with hidden dimension 32 followed by a small\n",
        "        Fully Connected netowrk that returns the Occupancy prediction.\n",
        "        Given the features vector in input, we have to perform Bilinear Interpolation before the ResNet blocks.\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT:\n",
        "\n",
        "                > Tensor of shape [input_cloud] (batch_size, n_points=1024, 3)\n",
        "                > Tensor of shape [features vector] (batch_size, features=32, resolution=64, resolution=64)\n",
        "\n",
        "\n",
        "            @ OUTPUT:\n",
        "\n",
        "                > Tensor of shape [occupancy prediction] (batch_size, n_points=1024, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=32, n_points=1024, n_blocks=5):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.res_blocks = nn.ModuleList(\n",
        "            [ResBlock(in_dim, n_points, in_dim, in_dim) for i in range(n_blocks)]\n",
        "        )\n",
        "        self.occupancy_pred = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_dim//2, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, input_features):\n",
        "        x = input\n",
        "\n",
        "        # Normalize your Input Cloud to 2D and Apply Bilinear Interpolation\n",
        "\n",
        "        norm_cloud = CoordinateNormalization(input)\n",
        "\n",
        "        # print(f\"norm_cloud : ({norm_cloud.shape}), unsqueeze : ({norm_cloud.unsqueeze(2).shape})\")\n",
        "\n",
        "        interpolated_features = F.grid_sample(input_features, norm_cloud.unsqueeze(2), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # print(f\"interpolated_features : ({interpolated_features.shape}), squeeze : ({interpolated_features.squeeze(-1).shape})\")\n",
        "\n",
        "        x = interpolated_features.squeeze(-1).transpose(2,1)\n",
        "\n",
        "        for res_block in self.res_blocks:\n",
        "            x = res_block(x)\n",
        "\n",
        "        occupancy = self.occupancy_pred(x)\n",
        "        # print(f\"occupancy : ({occupancy.shape})\")\n",
        "\n",
        "        print(f\"Decoder : ({input_features.shape}) --> ({occupancy.shape}) \\n\")\n",
        "\n",
        "        return occupancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbexQquN9WKi"
      },
      "source": [
        "# 4] Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9zVxRg_PxY"
      },
      "source": [
        "$\n",
        "CD(A, B) = \\frac{1}{|A|} \\sum_{a \\in A} \\min_{b \\in B} \\|a - b\\|_2^2 + \\frac{1}{|B|} \\sum_{b \\in B} \\min_{a \\in A} \\|b - a\\|_2^2\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vcU830ok9WKj"
      },
      "outputs": [],
      "source": [
        "# Chamster Distance\n",
        "def ChamsterDistance(input_cloud, pred_cloud):\n",
        "    \"\"\"\n",
        "        This function is used to compute the Chamster Distance between the input cloud and the predicted cloud.\n",
        "        This metrics is composed by sum of the distances from each point in input_cloud to its nearest neighbor in pred_cloud\n",
        "        plus the distances from each point in pred_cloud to its nearest neighbor in input_cloud.\n",
        "        We would like to MINIMIZE this metric.\n",
        "\n",
        "        @ INPUT :\n",
        "            > tensor of shape (batch_size, num_points, 3)\n",
        "            > tensor of shape (batch_size, num_points, 3)\n",
        "\n",
        "        @ OUTPUT :\n",
        "            > tensor of shape (batch_size, num_points, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    A = input_cloud.detach().cpu()\n",
        "    B = pred_cloud.detach().cpu()\n",
        "\n",
        "    dists_A_to_B = torch.cdist(A, B, p=2)\n",
        "    dists_B_to_A = torch.cdist(B, A, p=2)\n",
        "\n",
        "    print(f\"dists_A_to_B : ({dists_A_to_B.shape})\")\n",
        "    print(f\"dists_B_to_A : ({dists_B_to_A.shape})\")\n",
        "\n",
        "    min_A_to_B = torch.min(dists_A_to_B, dim=1)[0]\n",
        "    min_B_to_A = torch.min(dists_B_to_A, dim=1)[0]\n",
        "\n",
        "    print(f\"min_A_to_B : ({min_A_to_B.shape})\")\n",
        "    print(f\"min_B_to_A : ({min_B_to_A.shape})\")\n",
        "\n",
        "    CD = torch.mean(min_A_to_B) + torch.mean(min_B_to_A)\n",
        "\n",
        "    print(f\"Chamster Distance : ({CD.shape}) \\n\")\n",
        "\n",
        "    return CD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb7uqm2c_Wbf"
      },
      "source": [
        "$IoU(A', B') = \\frac{|A' \\cap B'|}{|A' \\cup B'|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qMwdO44rEyJl"
      },
      "outputs": [],
      "source": [
        "# Intersection Over Union\n",
        "def IntersectionOverUnion(input_cloud, pred_cloud):\n",
        "    \"\"\"\n",
        "        This function is used to compute the Intersection Over Union between the input cloud and the predicted cloud.\n",
        "\n",
        "        @ INPUT :\n",
        "            > tensor of shape (batch_size, num_points, 3)\n",
        "            > tensor of shape (batch_size, num_points, 3)\n",
        "\n",
        "        @ OUTPUT :\n",
        "            > tensor of shape (batch_size, num_points, 1)\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fQEisF9oIkWx"
      },
      "outputs": [],
      "source": [
        "# F-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wVAClDUMIl0T"
      },
      "outputs": [],
      "source": [
        "# Normal Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIpxjGH19WKj"
      },
      "source": [
        "# 5] Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=trainSetRandom, batch_size=1, shuffle=True)\n",
        "# val_loader ...\n",
        "# test_loader ..."
      ],
      "metadata": {
        "id": "VXKgijMIZbFT"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def OccupancyGroundTruth(sampled_points, registration_points, threshold=0.05):\n",
        "    \"\"\"\n",
        "        This function is used to obtain the occupancy ground truth from the registration of the input cloud.\n",
        "        This is crucial since it will be the quantity compared to the occupancy prediction.\n",
        "        The dataset provide the registration of the input cloud, which is the set of point on the surface of each figure.\n",
        "\n",
        "        @ INPUT :\n",
        "            > sampled_points: tensor of shape (batch_size, num_points=1024, 3)\n",
        "            > registration_points: tensor of shape (batch_size, num_points_reg , 3)\n",
        "            > threshold: float, default=0.05\n",
        "\n",
        "        @ OUTPUT :\n",
        "            > tensor of shape (batch_size, num_points, 1)\n",
        "    \"\"\"\n",
        "    # compute the distance between each of the sampled points and all the registration's points\n",
        "    dis = sampled_points[:, :, None, :] - registration_points[:, None, :, :]\n",
        "    distances = torch.norm(dis, dim=-1)\n",
        "    # find the minimum distances aka the distance between each point and its nearest neighbor\n",
        "    min_distances, _ = torch.min(distances, dim=2)\n",
        "    return (min_distances < threshold).float().unsqueeze(-1)"
      ],
      "metadata": {
        "id": "X4zji7u90ISp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jBGZGh8yBHMp"
      },
      "outputs": [],
      "source": [
        "class CompleteArchitecture(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=3, out_dim=32, n_points=1024, n_blocks=5):\n",
        "        super(CompleteArchitecture, self).__init__()\n",
        "        self.encoder = Encoder(in_dim, out_dim, n_points, n_blocks)\n",
        "        self.decoder = Decoder(out_dim, n_points, n_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_enc = self.encoder(x)\n",
        "        out_dec = self.decoder(x, out_enc)\n",
        "        return out_dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "heghvvVw9WKj"
      },
      "outputs": [],
      "source": [
        "completeModel = CompleteArchitecture()\n",
        "BCE = nn.BCELoss() # later change this and use the version with logits (remember to delete sigmoid in the occupancy pred network)\n",
        "optimizer = torch.optim.Adam(completeModel.parameters(), lr=1e-4) # currently using the same from the paper\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    completeModel.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    train_loss_cnt = []\n",
        "    val_loss_cnt = []\n",
        "\n",
        "    # add other metrics container (IOU, chamfer, ...)\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        sampled_cloud, ground_truth = batch\n",
        "        # use the gpu\n",
        "        occupancy_pred = completeModel(sampled_cloud)\n",
        "        # calculate the loss between the occupancy prediction and the real occupancy\n",
        "        real_occupancy =  OccupancyGroundTruth(sampled_cloud, ground_truth)\n",
        "\n",
        "        loss = BCE(occupancy_pred, real_occupancy)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        print(f\"current_train_loss: ({train_loss.item:.4f})\")\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "\n",
        "    completeModel.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            sampled_cloud, ground_truth = batch\n",
        "            # use the gpu\n",
        "            occupancy_pred = completeModel(sampled_cloud)\n",
        "            real_occupancy =  OccupancyGroundTruth(sampled_cloud, ground_truth)\n",
        "            loss = BCE(occupancy_pred, real_occupancy)\n",
        "            val_loss += loss.item()\n",
        "            # compute other metrics\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # save the model if it's better than the last one\n",
        "\n",
        "    \"\"\"\n",
        "    torch.save(\n",
        "        {\n",
        "        'model_state_dict': completeModel.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        },\n",
        "        'model.pt'\n",
        "    )\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u0r2Gpj9WKj"
      },
      "source": [
        "# 6] Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4ToDTMs9WKj"
      },
      "outputs": [],
      "source": [
        "# given the occupancy predictions of a small subset of points inside the scan, we would like to reconstruct the entire point cloud\n",
        "# The author of the paper have used this approach :\n",
        "\n",
        "\"\"\"\n",
        "    Multiresolution IsoSurface Extraction: We first mark all points at a given resolution which have already been evaluated as either occupied (red circles)\n",
        "    or unoccupied (cyan diamonds). We then determine all voxels that have both occupied and unoccupied corners and mark them as active (light red) and\n",
        "    subdivide them into 8 subvoxels each. Next, we evaluate all new grid points (empty circles) that have been introduced by the subdivision.\n",
        "    The previous two steps are repeated until the desired output resolution is reached. Finally we extract the mesh using the marching cubes algorithm [44],\n",
        "    simplify and refine the output mesh using first and second order gradient information.\n",
        "    We apply the Marching Cubes algorithm [44] to extract an approximate isosurface :\n",
        "        {p ∈ R3 | fθ(p,x) = τ}. (5)\n",
        "    Our algorithm converges to the correct mesh if the occupancy grid at the initial resolution contains points from every connected component of both the\n",
        "    interior and the exterior of the mesh. It is hence important to take an initial resolution which is high enough to satisfy this condition.\n",
        "    In practice, we found that an initial resolution of 323 was sufficient in almost all cases.\n",
        "    The initial mesh extracted by the Marching Cubes algorithm can be further refined. In a first step, we simplify the mesh using the\n",
        "    Fast-Quadric-Mesh-Simplification algorithm. Finally, we refine the output mesh using first and second order (i.e., gradient) information.\n",
        "    Towards this goal, we sample random points pk from each face of the output mesh and minimize the loss.\n",
        "\"\"\"\n",
        "def Reconstruction(occupancy_pred):\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9_0FERiAhMW8",
        "GFBi0FgT9WKe",
        "ibjyXRMqN7eo",
        "Rh8ugKUp9WKf",
        "8abAwINI9WKf",
        "Hk_D19aI9WKg",
        "QHoALeYs9WKg",
        "3BEqvWJV9WKh",
        "fME3ek-b9WKh",
        "_Xfwdl8S9WKi",
        "FbexQquN9WKi",
        "4u0r2Gpj9WKj"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}