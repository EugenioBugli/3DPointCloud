{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugenioBugli/PointCloud3D/blob/main/PointCloud3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_0FERiAhMW8"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kBbL55fBHVY2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q open3d\n",
        "!pip install -q torch_scatter\n",
        "!git clone https://github.com/EugenioBugli/PointCloud3D.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su0uRyWQE_On",
        "outputId": "3c3cce03-de56-462a-cc2b-e07753223167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PyTorch version: 2.5.1+cu121  Device: cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import tqdm\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_scatter import scatter_mean, scatter_max\n",
        "\n",
        "import open3d as o3d\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/PointCloud3D')\n",
        "from PointCloud3D.src.dataset import FAUST_Dataset, LoadDataset, openDataFiles\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "drive_path = \"/content/drive/MyDrive/CV/MPI-FAUST\"\n",
        "training_path = drive_path + \"/training\"\n",
        "test_path = drive_path + \"/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pJUMCQszcoPr"
      },
      "outputs": [],
      "source": [
        "SAMPLING_TYPE = \"RANDOM\"\n",
        "SAMPLING_SIZE = 2048\n",
        "BATCH_SIZE = 5\n",
        "\n",
        "IN_DIM_RES_PT = 64\n",
        "FEATURES_DIM = 32\n",
        "NUM_BLOCKS = 5\n",
        "NUM_FC = 4\n",
        "NUM_PLANES = 4\n",
        "NUM_COORDINATES = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFBi0FgT9WKe"
      },
      "source": [
        "# 1] Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2UANsdbf_on3"
      },
      "outputs": [],
      "source": [
        "# use this to directly import the already pre-processed dataset\n",
        "\n",
        "# train_scan_files, train_reg_files, val_scan_files, val_reg_files, test_scan_files, test_reg_files = openDataFiles(training_path, test_path, val_size=0.2)\n",
        "# train_set = FAUST_Dataset(train_scan_files, train_reg_files, sampling_type=SAMPLING_TYPE, sampling_size=SAMPLING_SIZE, partition=\"TRAIN\", transform=transforms.ToTensor())\n",
        "# val_set = FAUST_Dataset(val_scan_files, val_reg_files, sampling_type=SAMPLING_TYPE, sampling_size=SAMPLING_SIZE, partition=\"VAL\", transform=transforms.ToTensor())\n",
        "# test_set = FAUST_Dataset(test_scan_files, test_reg_files, sampling_type=SAMPLING_TYPE, sampling_size=SAMPLING_SIZE, partition=\"TEST\", transform=transforms.ToTensor())\n",
        "\n",
        "train_set = LoadDataset(\"train_set.pt\")\n",
        "val_set = LoadDataset(\"val_set.pt\")\n",
        "test_set = LoadDataset(\"test_set.pt\")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC0Pc51bA4sO"
      },
      "source": [
        "# 1.2] Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4MhWfu3WG3uF"
      },
      "outputs": [],
      "source": [
        "def Plot2D(cloud2d):\n",
        "    figure2D = plt.figure(figsize=(7,7))\n",
        "    axes2D = plt.axes()\n",
        "    axes2D.scatter(cloud2d[:, 0], cloud2d[:, 1], alpha=0.5)\n",
        "    plt.show()\n",
        "\n",
        "def voxel2Numpy(voxel):\n",
        "    voxels = voxel.get_voxels()\n",
        "    # notice that the coordinates are in the voxel grid !\n",
        "    indices = np.stack(list(vx.grid_index for vx in voxels))\n",
        "    return indices\n",
        "\n",
        "def PlotVoxel(input_cloud):\n",
        "    # input cloud must be np.array here\n",
        "    input_cloud = (input_cloud - input_cloud.min())/(input_cloud.max() - input_cloud.min() + 10e-6)\n",
        "    cloud = o3d.geometry.PointCloud()\n",
        "    cloud.points = o3d.utility.Vector3dVector(input_cloud)\n",
        "    o3d.visualization.draw_plotly([cloud])\n",
        "\n",
        "    cloud.scale(1 / np.max(cloud.get_max_bound() - cloud.get_min_bound()), center=cloud.get_center())\n",
        "\n",
        "    # try downsampling the point cloud that you have with 3D voxels\n",
        "    # all the points belonging to the cloud are bucketed into voxels and then each occupied voxel generates exactly one point by averaging all points inside\n",
        "\n",
        "    voxel = o3d.geometry.VoxelGrid.create_from_point_cloud(cloud, voxel_size=0.025)\n",
        "    vec3d = o3d.utility.Vector3dVector(input_cloud)\n",
        "    # use this to check if your points are inside the voxel\n",
        "    print(np.asarray(voxel.check_if_included(vec3d)))\n",
        "\n",
        "    print((voxel.get_voxels()[0]).grid_index)\n",
        "    fig = go.Figure()\n",
        "    voxel = np.asarray(voxel.get_voxels())\n",
        "    points = np.asarray([v.grid_index for v in voxel])\n",
        "    fig.add_trace(\n",
        "        go.Scatter3d(\n",
        "            x=points[:, 0], y=points[:, 1], z=points[:, 2],\n",
        "            mode='markers',\n",
        "            marker=dict(size=3, color='green', opacity=0.5)\n",
        "    ))\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHwmXadfmd4Q"
      },
      "outputs": [],
      "source": [
        "# add noise to a point cloud:\n",
        "for batch in train_loader:\n",
        "    eg = batch[0][0]\n",
        "    eg = (eg - eg.min())/(eg.max() - eg.min() + 10e-6)\n",
        "\n",
        "    pc = o3d.geometry.PointCloud()\n",
        "    pc.points = o3d.utility.Vector3dVector(eg)\n",
        "    o3d.visualization.draw_plotly([pc])\n",
        "\n",
        "    eg_noise = eg + np.random.normal(0, 0.01, eg.shape) # gaussian noise\n",
        "    pc_noise = o3d.geometry.PointCloud()\n",
        "    pc_noise.points = o3d.utility.Vector3dVector(eg_noise)\n",
        "    o3d.visualization.draw_plotly([pc_noise])\n",
        "\n",
        "    pc.scale(1 / np.max(pc.get_max_bound() - pc.get_min_bound()), center=pc.get_center())\n",
        "\n",
        "    # try downsampling the point cloud that you have with 3D voxels\n",
        "    # all the points belonging to the cloud are bucketed into voxels and then each occupied voxel generates exactly one point by averaging all points inside.\n",
        "    PlotVoxel(eg)\n",
        "\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh8ugKUp9WKf"
      },
      "source": [
        "# 2] Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8abAwINI9WKf"
      },
      "source": [
        "## 2.1) ResBlock + Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OZfg7nQw9WKf"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define a Residual Block, which is one of the main component of the ResNetPointNet architecture\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=64, n_points=2048, h_dim=32, out_dim=64):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_points = n_points\n",
        "\n",
        "        #> First part of the Block\n",
        "\n",
        "        self.fc1 = nn.Linear(\n",
        "            in_dim,\n",
        "            h_dim\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(self.n_points)\n",
        "\n",
        "        #> Second part of the Block\n",
        "\n",
        "        self.fc2 = nn.Linear(\n",
        "            h_dim,\n",
        "            out_dim\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(self.n_points)\n",
        "\n",
        "        #> Skip connection\n",
        "\n",
        "        if in_dim != out_dim:\n",
        "            # size mismatch (never happen in my case)\n",
        "            self.residual = nn.Linear(in_dim, out_dim)\n",
        "        else:\n",
        "            # same size\n",
        "            self.residual = None\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (b,p,in_dim) = (b,p,64)\n",
        "\n",
        "        first_part = F.relu(self.bn1(self.fc1(x))) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        second_part = self.bn2(self.fc2(first_part)) # (b,p,32) -> (b,p,64)\n",
        "\n",
        "        if self.residual is None:\n",
        "            # no size mismatch\n",
        "            res = x # (b,p,64)\n",
        "        else:\n",
        "            # transformation if there is a size mismatch in_dim != out_dim\n",
        "            res = self.residual(x) # (b,p,in_dim) -> (b,p,64)\n",
        "\n",
        "        # add residual connection\n",
        "        third_part = second_part + res # (b,p,64) -> (b,p,64)\n",
        "\n",
        "        return F.relu(third_part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk_D19aI9WKg"
      },
      "source": [
        "## 2.2) ResNetPointNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "v_Do5AElGE7k"
      },
      "outputs": [],
      "source": [
        "def CloudNormalization(input_cloud):\n",
        "    # normalize cloud in order to have it in [0,1]\n",
        "    min = input_cloud.min()\n",
        "    max = input_cloud.max()\n",
        "    norm_cloud = (input_cloud - min)/(max-min)\n",
        "    return norm_cloud\n",
        "\n",
        "def CanonicalProjection(input_cloud):\n",
        "    # gives you a dict with all 3 canonical projections (xy, xz, yz)\n",
        "    xy_proj = CloudNormalization(input_cloud[:,:,[0,1]]) # XY plane (view from above)\n",
        "    xz_proj = CloudNormalization(input_cloud[:,:,[0,2]]) # XZ plane (view from long side)\n",
        "    yz_proj = CloudNormalization(input_cloud[:,:,[1,2]]) # YZ plane (view from short side)\n",
        "    return {\"xy\": xy_proj, \"xz\": xz_proj, \"yz\": yz_proj}\n",
        "\n",
        "def DivideInBuckets(input_cloud_proj, resolution):\n",
        "    # in input we have the canonical projection of the input cloud and the resolution of our buckets\n",
        "    # map 2D coordinates into one dimension\n",
        "    xy_buck = (input_cloud_proj[\"xy\"]*resolution).long()\n",
        "    xz_buck = (input_cloud_proj[\"xz\"]*resolution).long()\n",
        "    yz_buck = (input_cloud_proj[\"yz\"]*resolution).long()\n",
        "    return {\"xy\": xy_buck[:,:,0] + resolution*xy_buck[:,:,1], \"xz\": xz_buck[:,:,0] + resolution*xz_buck[:,:,1], \"yz\": yz_buck[:,:,0] + resolution*yz_buck[:,:,1]}\n",
        "\n",
        "def Plot2DWithBuckets(input_plane, resolution):\n",
        "    figure2D = plt.figure(figsize=(7,7))\n",
        "    axes2D = plt.axes()\n",
        "    axes2D.scatter(input_plane[:, 0], input_plane[:, 1], alpha=0.5)\n",
        "\n",
        "    for i in range(resolution + 1):\n",
        "        plt.axvline(x=i/resolution, color='gray', linestyle='--', linewidth=0.5)\n",
        "        plt.axhline(y=i/resolution, color='gray', linestyle='--', linewidth=0.5)\n",
        "    for xi in range(resolution):\n",
        "        for yi in range(resolution):\n",
        "            bucket_idx = xi + resolution*yi\n",
        "            # Center of the cell\n",
        "            x_center = (xi + 0.5)/resolution\n",
        "            y_center = (yi + 0.5)/resolution\n",
        "            plt.text(x_center, y_center, str(bucket_idx), fontsize=8, color='red', ha='center', va='center')\n",
        "\n",
        "    axes2D.set_xlim(0,1)\n",
        "    axes2D.set_ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "def LocalMaxPooling(input_features, input_cloud, resolution):\n",
        "    # given the 3D point cloud I need to project them into a 2D grid. In this way I can define the neighborhood to perform local pooling\n",
        "    # voxelization is not a good idea since I will lose details (all points are averaged into a voxel)\n",
        "    canon_proj = CanonicalProjection(input_cloud)\n",
        "    # we need to divide this 2D planes into buckets and the perform inside each of them the pooling operation\n",
        "    indices = DivideInBuckets(canon_proj, resolution) # only the full buckets are taken from the resolution^2 buckets\n",
        "    pooled_dict = dict()\n",
        "    for plane in canon_proj.keys():\n",
        "        projection_indices = indices[plane]\n",
        "        # max pooling operation over the input_features (batch, num_points, 32)\n",
        "        scatter_container, _ = scatter_max(input_features, projection_indices.squeeze(-1), dim=0, dim_size=input_cloud.shape[0]*input_cloud.shape[1])\n",
        "        print(scatter_container.shape)\n",
        "        pooled_dict[plane] = scatter_container\n",
        "    return scatter_container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "qYhVqWYM9WKg"
      },
      "outputs": [],
      "source": [
        "class ResNetPointNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the PointNet model used to form a feature embedding for each point in the Point Cloud given in input.\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3)\n",
        "\n",
        "            > Fully Connected Layer (3, in_dim=64)\n",
        "            > 5 Residual Blocks with Local Pooling and Concatenation\n",
        "            > Fully Connected Layer (out_dim=32, out_dim=32)\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=64, n_points=2048, res_dim=32, out_dim=32, n_blocks=5):\n",
        "        super(ResNetPointNet, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(3, in_dim)\n",
        "\n",
        "        self.res = nn.ModuleList([\n",
        "            ResBlock(in_dim, n_points, res_dim, out_dim) for n_res in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (b,p,3)\n",
        "\n",
        "        # Extract Normalized coordinates and indices to perform local pooling\n",
        "\n",
        "        norm_coord = CanonicalProjection(x) # (b,p,2)\n",
        "        coord_indices = DivideInBuckets(norm_coord, 32) # (b,p,2)\n",
        "\n",
        "        # First FC Layer\n",
        "        fc1 = F.relu(self.fc1(x)) # (b,p,3) -> (b,p,64)\n",
        "        print(\"fc1: \",fc1.shape)\n",
        "        # First ResBlock\n",
        "        res = self.res[0](fc1) # (b,p,64) -> (b,p,32)\n",
        "        print(\"res: \",res.shape)\n",
        "        # 2-5 ResBlock\n",
        "        for res_block in self.res[1:]:\n",
        "            # Local Pooling\n",
        "            pool = LocalMaxPooling(res, x, 32) # (b,p,32)\n",
        "            print(\"pool: \",pool.shape)\n",
        "            # Concatenation\n",
        "            concat = torch.cat([res, pool], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "            print(\"concatenation: \",concat.shape)\n",
        "            # following residual block\n",
        "            res = res_block(concat) # (b,p,64) -> (b,p,32)\n",
        "\n",
        "        # Last FC Layer\n",
        "        final = F.relu(self.fc2(res)) # (b,p,32) -> (b,p,32)\n",
        "\n",
        "        #   print(f\"ResNetPointNet : input ({x.shape}) ---> output ({final.shape}) \\n\")\n",
        "\n",
        "        return final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = ResNetPointNet()\n",
        "for batch in train_loader:\n",
        "    print(batch[0].shape)\n",
        "    print(batch[0][:,:,[0,1]].shape)\n",
        "    out = resnet(batch[0])\n",
        "    print(out.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "C2OTkZRs2jiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHoALeYs9WKg"
      },
      "source": [
        "## 2.3) Plane Predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASem9l-E9WKg"
      },
      "outputs": [],
      "source": [
        "class SimplePointNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define a simple variant of the PointNet Model, which is one of the main components of the Plane Predictor Network.\n",
        "        This Network will provide us a global context of the Input Point Clouds\n",
        "        Architecture Design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3) which represent the Input Point Clouds\n",
        "\n",
        "            > Fully Connected Layer (3, 64)\n",
        "\n",
        "              |> Fully Connected Layer (64, 32)\n",
        "            2*|> Global Max Pooling\n",
        "              |> Concatenation btw Pooled and unpooled features\n",
        "\n",
        "            > Fully Connected Layer (64, 32)\n",
        "            > Global Max Pooling\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32) which will be used by the rest of the Plane Predictor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, in_dim=64, n_points=1024, hid_dim=32, out_dim=64):\n",
        "        super(SimplePointNet, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_points = n_points\n",
        "\n",
        "        self.initial_fc = nn.Linear(in_features=3, out_features=in_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.final_fc = nn.Linear(in_features=in_dim, out_features=hid_dim)\n",
        "\n",
        "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.initial_fc(x)) # (b,p,3) --> (b,p,64)\n",
        "\n",
        "        x1 = F.relu(self.fc1(x)) # (b,p,64) --> (b,p,32)\n",
        "        x1_t = x1.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        pool_x1 = self.pool(x1_t) # (b,32,p) -> (b,32,1)\n",
        "        exp_pool_x1 = pool_x1.transpose(1,2).expand(self.batch_size, self.n_points, 32) # (b,32,1) -> (b,1,32) -> (b,p,32)\n",
        "        concat_x1 = torch.cat([x1, exp_pool_x1], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "\n",
        "        x2 = F.relu(self.fc2(concat_x1)) # (b,p,64) -> (b,p,32)\n",
        "        x2_t = x2.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        pool_x2 = self.pool(x2_t) # (b,32,p) -> (b,32,1)\n",
        "        exp_pool_x2 = pool_x2.transpose(1,2).expand(self.batch_size, self.n_points, 32) # (b,32,1) -> (b,1,32) -> (b,p,32)\n",
        "        concat_x2 = torch.cat([x2, exp_pool_x2], dim=2) # (b,p,32) | (b,p,32) -> (b,p,64)\n",
        "\n",
        "        pre_pool_out = F.relu(self.final_fc(concat_x2)) # (b,p,64) -> (b,p,32)\n",
        "        pre_pool_out_t = pre_pool_out.transpose(1,2) # (b,p,32) -> (b,32,p)\n",
        "        out = self.pool(pre_pool_out_t) # (b,32,p) -> (b,32,1)\n",
        "\n",
        "        #   print(f\"SimplePointNet : input ({x.shape}) ---> output ({out.transpose(1,2).shape}) \\n\")\n",
        "        return out.transpose(1,2) # (b,32,1) -> (b,1,32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cWO2Alj9WKg"
      },
      "outputs": [],
      "source": [
        "class PlanePredictor(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the Plane Predictor of our Architecture, which will predict the plane parameters of L dynamic planes\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT: Tensor of shape (batch_size, num_points, 3) which represent Point Clouds\n",
        "\n",
        "            > Simple PointNet which learns the global context of the input point clouds\n",
        "            > This information is encoded into one global feature by using Max Pooling\n",
        "            > 4 Fully Connected Layers with hidden dimension = 32\n",
        "            > L Shallow Networks with hidden dimension = 3 which will give us the Predicted Plane Parameters\n",
        "            > L Fully Connected Layers with 1 layer and hidden dimension = D (same as point cloud encoder hidden dimension)\n",
        "            > Each plane-specific feature is expanded to N x D to match the output of the point cloud encoder, which will be summed together\n",
        "\n",
        "\n",
        "            @ OUTPUT: Tensor of shape (batch_size, num_points, 32) which will be summed up to the features given by the ResNetPointNet before\n",
        "                      being processed into U-Net\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=32, n_points=1024, n_fc=4, L=4):\n",
        "        super(PlanePredictor, self).__init__()\n",
        "\n",
        "        self.pointNet = SimplePointNet(batch_size=BATCH_SIZE, in_dim=IN_DIM_RES_PT, n_points=n_points, hid_dim=in_dim, out_dim=IN_DIM_RES_PT)\n",
        "        self.n_points = n_points\n",
        "\n",
        "        # 4 FC layers with hidden dim = 32\n",
        "\n",
        "        self.four_fc = nn.ModuleList(\n",
        "            [nn.Linear(in_dim, in_dim) for i in range(n_fc)]\n",
        "        )\n",
        "\n",
        "        # Plane parameters (L shallow networks with hidden dim = 3)\n",
        "\n",
        "        self.first_shallow = nn.Linear(in_dim, 3)\n",
        "        self.shallows = nn.ModuleList(\n",
        "            [nn.Linear(3, 3) for i in range(L-1)]\n",
        "        )\n",
        "\n",
        "        # L FC layers with hidden dim = 32\n",
        "\n",
        "        self.first_fc = nn.Linear(3, in_dim)\n",
        "        self.L_fc = nn.ModuleList(\n",
        "            [nn.Linear(in_dim, in_dim) for i in range(L-1)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        flc = self.pointNet(x) # (b,p,3) -> (b,1,32)\n",
        "\n",
        "        # 4 FC layers with hidden dim = 32\n",
        "\n",
        "        for fc in self.four_fc:\n",
        "            flc = F.relu(fc(flc))  # (b,1,32) -> (b,1,32)\n",
        "\n",
        "        # Plane parameters ( L Shallow Networks )\n",
        "\n",
        "        first_sh = F.relu(self.first_shallow(flc)) # (b,1,32) -> (b,1,3)\n",
        "        shal = first_sh\n",
        "        for s in self.shallows:\n",
        "            shal = F.relu(s(shal)) # (b,1,3) -> (b,1,3)\n",
        "\n",
        "        # L FC networks dim = 32\n",
        "\n",
        "        first_fc_L = F.relu(self.first_fc(shal)) # (b,1,3) -> (b,1,32)\n",
        "        L_fully = first_fc_L\n",
        "        for fc in self.L_fc:\n",
        "            L_fully = F.relu(fc(L_fully))  # (b,1,32) -> (b,1,32)\n",
        "\n",
        "        # Expansion\n",
        "\n",
        "        out = L_fully.expand(x.shape[0], self.n_points, first_fc_L.shape[-1]) # (b,1,32) -> (b,p,32)\n",
        "\n",
        "        #   print(f\"PlanePredictor : input ({x.shape}) ---> output ({out.shape}) \\n\")\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XZAjemIsD0r"
      },
      "source": [
        "## 2.4) Construct Projected Feature Planes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erplhNg6SWjx"
      },
      "source": [
        "$R = I + skew(v) + skew(v)^2 \\cdot \\frac{1-\\mathcal{k} \\cdot \\mathcal{\\hat{n}}} {||v||^2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuX--AT7sL3j"
      },
      "outputs": [],
      "source": [
        "# we need to project the encoded features (the output of the previous step aka 2.2+2.3) onto the dynamic planes with a defined size of H x W grid and then\n",
        "# apply max-pooling for the features inside each cell\n",
        "# 3 different operations must be employed to keep them inside H x W grids.\n",
        "# > Basis Change\n",
        "# > Orthographic Projection (map 3D points into 2D plane coordinates)\n",
        "# > Normalization\n",
        "\n",
        "def ChangeOfBasis():\n",
        "    # reorientation of the coordinate system to align it with the dynamic plane (needed if you are not using the 3 canonical planes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BEqvWJV9WKh"
      },
      "source": [
        "## 2.5) UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVGhQD969WKh"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the UNet of our Architecture, which is the final part of our Encoder.\n",
        "        Architecture design:\n",
        "\n",
        "        @ INPUT: Tensor of shape (batch_size, 32, 64, 64)\n",
        "            # Encoder:\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > MaxPool2D\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > MaxPool2D\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > MaxPool2D\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "            # BottleNeck:\n",
        "\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "            # Decoder:\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > UpConv2D\n",
        "                > Concat\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "                > Conv2D\n",
        "                > BatchNorm\n",
        "                > ReLU\n",
        "\n",
        "                > Conv2D\n",
        "\n",
        "        @ OUTPUT: Tensor of shape (batch_size, num_points, 32)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim=32, out_dim=32, features_dim=64, n_points=1024):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        # Encoder\n",
        "\n",
        "        ## Block 1\n",
        "\n",
        "        self.e_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_dim,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_1 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        ## Block 2\n",
        "\n",
        "        self.e_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_2 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        ## Block 3\n",
        "\n",
        "        self.e_3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_3 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        ## Block 4\n",
        "\n",
        "        self.e_4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.p_4 = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2\n",
        "        )\n",
        "\n",
        "        # Bottleneck\n",
        "\n",
        "        self.b = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*16,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*16),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*16,\n",
        "                out_channels=features_dim*16,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*16),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "\n",
        "        ## Block 1\n",
        "\n",
        "        self.d_upconv1 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*16,\n",
        "            out_channels=features_dim*8,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*16,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*8,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*8),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        ## Block 2\n",
        "\n",
        "        self.d_upconv2 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*8,\n",
        "            out_channels=features_dim*4,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*8,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*4,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*4),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        ## Block 3\n",
        "\n",
        "        self.d_upconv3 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*4,\n",
        "            out_channels=features_dim*2,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*4,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim*2,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim*2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        ## Block 4\n",
        "\n",
        "        self.d_upconv4 = nn.ConvTranspose2d(\n",
        "            in_channels=features_dim*2,\n",
        "            out_channels=features_dim,\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "\n",
        "        self.d_4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim*2,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=features_dim,\n",
        "                kernel_size=3,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(num_features=features_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(\n",
        "                in_channels=features_dim,\n",
        "                out_channels=out_dim,\n",
        "                kernel_size=1\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        enc1 = self.e_1(x)\n",
        "        #print(f\"Encoder block 1 : input ({x.shape}) ---> output ({enc1.shape})\")\n",
        "        enc2 = self.e_2(self.p_1(enc1))\n",
        "        #print(f\"Encoder block 2 : input ({self.p_1(enc1).shape}) ---> output ({enc2.shape})\")\n",
        "        enc3 = self.e_3(self.p_2(enc2))\n",
        "        #print(f\"Encoder block 3 : input ({self.p_2(enc2).shape}) ---> output ({enc3.shape})\")\n",
        "        enc4 = self.e_4(self.p_3(enc3))\n",
        "        #print(f\"Encoder block 4 : input ({self.p_3(enc3).shape}) ---> output ({enc4.shape})\")\n",
        "\n",
        "        bottle = self.b(self.p_4(enc4))\n",
        "        #print(f\"BottleNeck : input ({self.p_4(enc4).shape}) ---> output ({bottle.shape})\")\n",
        "\n",
        "        dec1 = self.d_upconv1(bottle)\n",
        "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
        "        dec1 = self.d_1(dec1)\n",
        "        #print(f\"Decoder block 1 : input ({bottle.shape}) ---> output ({dec1.shape})\")\n",
        "        dec2 = self.d_upconv2(dec1)\n",
        "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
        "        dec2 = self.d_2(dec2)\n",
        "        #print(f\"Decoder block 2 : input ({dec1.shape}) ---> output ({dec2.shape})\")\n",
        "        dec3 = self.d_upconv3(dec2)\n",
        "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
        "        dec3 = self.d_3(dec3)\n",
        "        #print(f\"Decoder block 3 : input ({dec2.shape}) ---> output ({dec3.shape})\")\n",
        "        dec4 = self.d_upconv4(dec3)\n",
        "        dec4 = torch.cat((dec4, enc1), dim=1)\n",
        "        dec4 = self.d_4(dec4)\n",
        "        #print(f\"Decoder block 4 : input ({dec3.shape}) ---> output ({dec4.shape})\")\n",
        "        out = self.final(dec4)\n",
        "        #print(f\"Final : input ({dec4.shape}) ---> output ({out.shape})\")\n",
        "\n",
        "\n",
        "        #   print(f\"UNet : input ({x.shape}) ---> output ({out.shape}) \\n\")\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fME3ek-b9WKh"
      },
      "source": [
        "## 2.6) Complete Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEnfYEQZ9WKh"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=64, out_dim=32, n_points=1024, n_blocks=5, num_planes=4, num_fc=4):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.resnet_pointnet = ResNetPointNet(in_dim=in_dim, n_points=n_points, out_dim=out_dim, n_blocks=n_blocks)\n",
        "        self.plane_predictor = PlanePredictor(in_dim=out_dim, n_points=n_points, n_fc=num_fc, L=num_planes)\n",
        "        self.UNet = UNet(in_dim=out_dim, out_dim=out_dim, features_dim=in_dim, n_points=n_points)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        resnet = self.resnet_pointnet(x)\n",
        "        plane_pred = self.plane_predictor(x)\n",
        "        # here we need to perform features projection\n",
        "        features_proj = # something(x, resnet + plane_pred)\n",
        "        out = self.UNet(features_proj)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xfwdl8S9WKi"
      },
      "source": [
        "# 3] Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Cu6WzB9WKi"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "        This class is used to define the Decoder Network. The architecture is composed by 5 ResNet blocks with hidden dimension 32 followed by a small\n",
        "        Fully Connected netowrk that returns the Occupancy prediction.\n",
        "        Given the features vector in input, we have to perform Bilinear Interpolation before the ResNet blocks.\n",
        "        Architecture design:\n",
        "\n",
        "            @ INPUT:\n",
        "\n",
        "                > Tensor of shape [input_cloud] (batch_size, n_points=1024, 3)\n",
        "                > Tensor of shape [features vector] (batch_size, features=32, resolution=64, resolution=64)\n",
        "\n",
        "\n",
        "            @ OUTPUT:\n",
        "\n",
        "                > Tensor of shape [occupancy prediction] (batch_size, n_points=1024, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_dim=32, n_points=1024, n_blocks=5):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.res_blocks = nn.ModuleList(\n",
        "            [ResBlock(in_dim, n_points, in_dim, in_dim) for i in range(n_blocks)]\n",
        "        )\n",
        "        self.occupancy_pred = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_dim//2, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input, input_features):\n",
        "        x = input\n",
        "\n",
        "        # Normalize your Input Cloud to 2D and Apply Bilinear Interpolation\n",
        "\n",
        "        norm_cloud = CoordinateNormalization(input)\n",
        "\n",
        "        # print(f\"norm_cloud : ({norm_cloud.shape}), unsqueeze : ({norm_cloud.unsqueeze(2).shape})\")\n",
        "\n",
        "        interpolated_features = F.grid_sample(input_features, norm_cloud.unsqueeze(2), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # print(f\"interpolated_features : ({interpolated_features.shape}), squeeze : ({interpolated_features.squeeze(-1).shape})\")\n",
        "\n",
        "        x = interpolated_features.squeeze(-1).transpose(2,1)\n",
        "\n",
        "        for res_block in self.res_blocks:\n",
        "            x = res_block(x)\n",
        "\n",
        "        occupancy = self.occupancy_pred(x)\n",
        "        # print(f\"occupancy : ({occupancy.shape})\")\n",
        "\n",
        "        #   print(f\"Decoder : ({input_features.shape}) --> ({occupancy.shape}) \\n\")\n",
        "\n",
        "        return occupancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbexQquN9WKi"
      },
      "source": [
        "# 4] Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh9zVxRg_PxY"
      },
      "source": [
        "<font color=\"orange\"> Chamster Distance: </font> $\n",
        "CD(A, B) = \\frac{1}{|A|} \\sum_{a \\in A} \\min_{b \\in B} \\|a - b\\|_2^2 + \\frac{1}{|B|} \\sum_{b \\in B} \\min_{a \\in A} \\|b - a\\|_2^2\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcU830ok9WKj"
      },
      "outputs": [],
      "source": [
        "# Chamster Distance\n",
        "def ChamsterDistance(input_cloud, pred_cloud):\n",
        "    \"\"\"\n",
        "        This function is used to compute the Chamster Distance between the input cloud and the predicted cloud.\n",
        "        This metrics is composed by sum of the distances from each point in input_cloud to its nearest neighbor in pred_cloud\n",
        "        plus the distances from each point in pred_cloud to its nearest neighbor in input_cloud.\n",
        "        We would like to MINIMIZE this metric.\n",
        "\n",
        "        @ INPUT :\n",
        "            > tensor of shape (batch_size, num_points, 3)\n",
        "            > tensor of shape (batch_size, num_points, 3)\n",
        "\n",
        "        @ OUTPUT :\n",
        "            > tensor of shape (batch_size, num_points, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    A = input_cloud.detach().cpu()\n",
        "    B = pred_cloud.detach().cpu()\n",
        "\n",
        "    dists_A_to_B = torch.cdist(A, B, p=2)\n",
        "    dists_B_to_A = torch.cdist(B, A, p=2)\n",
        "\n",
        "    print(f\"dists_A_to_B : ({dists_A_to_B.shape})\")\n",
        "    print(f\"dists_B_to_A : ({dists_B_to_A.shape})\")\n",
        "\n",
        "    min_A_to_B = torch.min(dists_A_to_B, dim=1)[0]\n",
        "    min_B_to_A = torch.min(dists_B_to_A, dim=1)[0]\n",
        "\n",
        "    print(f\"min_A_to_B : ({min_A_to_B.shape})\")\n",
        "    print(f\"min_B_to_A : ({min_B_to_A.shape})\")\n",
        "\n",
        "    CD = torch.mean(min_A_to_B) + torch.mean(min_B_to_A)\n",
        "\n",
        "    print(f\"Chamster Distance : ({CD.shape}) \\n\")\n",
        "\n",
        "    return CD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb7uqm2c_Wbf"
      },
      "source": [
        "<font color=\"orange\"> Volumetric IOU: </font> $ IoU(A', B') = \\frac{|A' \\cap B'|}{|A' \\cup B'|}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMwdO44rEyJl"
      },
      "outputs": [],
      "source": [
        "# Intersection Over Union\n",
        "def VolumetricIOU(real_cloud, pred_cloud):\n",
        "    \"\"\"\n",
        "        This function is used to compute the Intersection Over Union between two volumes\n",
        "\n",
        "        @ INPUT :\n",
        "            > tensor of shape (batch_size, ...) (actually binary voxel grid)\n",
        "            > tensor of shape (batch_size, ...) (actually binary voxel grid)\n",
        "\n",
        "        @ OUTPUT :\n",
        "            > float\n",
        "    \"\"\"\n",
        "    real_cloud = np.array(real_cloud, dtype=np.bool)\n",
        "    pred_cloud = np.array(pred_cloud, dtype=np.bool)\n",
        "\n",
        "    return np.sum(real_cloud & pred_cloud) / np.sum(pred_cloud) + np.sum(real_cloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyvI3CLJJ3Re"
      },
      "source": [
        "<font color=\"orange\"> F-Score: </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQEisF9oIkWx"
      },
      "outputs": [],
      "source": [
        "# F-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cBjgw_UJ5_R"
      },
      "source": [
        "<font color=\"orange\"> Normal Consistency: </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVAClDUMIl0T"
      },
      "outputs": [],
      "source": [
        "# Normal Consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIpxjGH19WKj"
      },
      "source": [
        "# 5] Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4zji7u90ISp"
      },
      "outputs": [],
      "source": [
        "def OccupancyGroundTruth(sampled_points, registration_points, threshold=0.05):\n",
        "    \"\"\"\n",
        "        This function is used to obtain the occupancy ground truth from the registration of the input cloud.\n",
        "        This is crucial since it will be the quantity compared to the occupancy prediction.\n",
        "        The dataset provide the registration of the input cloud, which is the set of point on the surface of each figure.\n",
        "\n",
        "        @ INPUT :\n",
        "            > sampled_points: tensor of shape (batch_size, num_points=1024, 3)\n",
        "            > registration_points: tensor of shape (batch_size, num_points_reg , 3)\n",
        "            > threshold: float, default=0.05\n",
        "\n",
        "        @ OUTPUT :\n",
        "            > tensor of shape (batch_size, num_points, 1)\n",
        "    \"\"\"\n",
        "    # compute the distance between each of the sampled points and all the registration's points\n",
        "    dis = sampled_points[:, :, None, :] - registration_points[:, None, :, :]\n",
        "    distances = torch.norm(dis, dim=-1)\n",
        "    # find the minimum distances aka the distance between each point and its nearest neighbor\n",
        "    min_distances, _ = torch.min(distances, dim=2)\n",
        "    return (min_distances < threshold).float().unsqueeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBGZGh8yBHMp"
      },
      "outputs": [],
      "source": [
        "class CompleteArchitecture(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim=64, out_dim=32, n_points=1024, n_blocks=5, num_planes=4, num_fc=4):\n",
        "        super(CompleteArchitecture, self).__init__()\n",
        "        self.encoder = Encoder(in_dim, out_dim, n_points, n_blocks, num_planes, num_fc)\n",
        "        self.decoder = Decoder(out_dim, n_points, n_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_enc = self.encoder(x)\n",
        "        out_dec = self.decoder(x, out_enc)\n",
        "        return out_dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heghvvVw9WKj",
        "outputId": "c2a43b1c-2bc0-4b1c-ac55-a9d54bf420d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/25], Train Loss: 0.5437, Val Loss: 0.5872\n",
            "Epoch [2/25], Train Loss: 0.3200, Val Loss: 0.4081\n",
            "Epoch [3/25], Train Loss: 0.2150, Val Loss: 0.2421\n",
            "Epoch [4/25], Train Loss: 0.1468, Val Loss: 0.1307\n",
            "Epoch [5/25], Train Loss: 0.0986, Val Loss: 0.0489\n",
            "Epoch [6/25], Train Loss: 0.0652, Val Loss: 0.0502\n",
            "Epoch [7/25], Train Loss: 0.0451, Val Loss: 0.0298\n",
            "Epoch [8/25], Train Loss: 0.0299, Val Loss: 0.0244\n",
            "Epoch [9/25], Train Loss: 0.0217, Val Loss: 0.0205\n",
            "Epoch [10/25], Train Loss: 0.0155, Val Loss: 0.0192\n",
            "Epoch [11/25], Train Loss: 0.0123, Val Loss: 0.0108\n",
            "Epoch [12/25], Train Loss: 0.0096, Val Loss: 0.0105\n",
            "Epoch [13/25], Train Loss: 0.0078, Val Loss: 0.0103\n",
            "Epoch [14/25], Train Loss: 0.0062, Val Loss: 0.0078\n",
            "Epoch [15/25], Train Loss: 0.0056, Val Loss: 0.0059\n",
            "Epoch [16/25], Train Loss: 0.0047, Val Loss: 0.0061\n",
            "Epoch [17/25], Train Loss: 0.0040, Val Loss: 0.0062\n",
            "Epoch [18/25], Train Loss: 0.0034, Val Loss: 0.0041\n",
            "Epoch [19/25], Train Loss: 0.0032, Val Loss: 0.0050\n",
            "Epoch [20/25], Train Loss: 0.0027, Val Loss: 0.0038\n",
            "Epoch [21/25], Train Loss: 0.0025, Val Loss: 0.0033\n",
            "Epoch [22/25], Train Loss: 0.0023, Val Loss: 0.0030\n",
            "Epoch [23/25], Train Loss: 0.0020, Val Loss: 0.0028\n",
            "Epoch [24/25], Train Loss: 0.0019, Val Loss: 0.0029\n",
            "Epoch [25/25], Train Loss: 0.0018, Val Loss: 0.0031\n"
          ]
        }
      ],
      "source": [
        "completeModel = CompleteArchitecture(in_dim=IN_DIM_RES_PT, out_dim=FEATURES_DIM, n_points=SAMPLING_SIZE, n_blocks=NUM_BLOCKS, num_planes=NUM_PLANES, num_fc=NUM_FC)\n",
        "completeModel = completeModel.to(device)\n",
        "BCE = nn.BCELoss() # later change this and use the version with logits (remember to delete sigmoid in the occupancy pred network)\n",
        "optimizer = torch.optim.Adam(completeModel.parameters(), lr=1e-4) # currently using the same from the paper\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    completeModel.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "\n",
        "    train_loss_cnt = []\n",
        "    val_loss_cnt = []\n",
        "\n",
        "    # add other metrics container (IOU, chamfer, ...)\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        sampled_cloud, ground_truth = batch[0].to(device), batch[1].to(device)\n",
        "        # use the gpu\n",
        "        occupancy_pred = completeModel(sampled_cloud)\n",
        "        # calculate the loss between the occupancy prediction and the real occupancy\n",
        "        real_occupancy =  OccupancyGroundTruth(sampled_cloud, ground_truth)\n",
        "\n",
        "        loss = BCE(occupancy_pred, real_occupancy)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    train_loss_cnt.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    completeModel.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            sampled_cloud, ground_truth = batch[0].to(device), batch[1].to(device)\n",
        "            # use the gpu\n",
        "            occupancy_pred = completeModel(sampled_cloud)\n",
        "            real_occupancy = OccupancyGroundTruth(sampled_cloud, ground_truth)\n",
        "            loss = BCE(occupancy_pred, real_occupancy)\n",
        "            val_loss += loss.item()\n",
        "            # perform the reconstruction\n",
        "            # meshes = Reconstruction(occupancy_pred, sampled_cloud)\n",
        "            # compute other metrics\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_loss_cnt.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "    # save the model if it's better than the last one\n",
        "\n",
        "    \"\"\"\n",
        "    torch.save(\n",
        "        {\n",
        "        'model_state_dict': completeModel.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        },\n",
        "        'model.pt'\n",
        "    )\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypdSm5bEsFIp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_cnt, label='Train')\n",
        "plt.plot(val_loss_cnt, label='Val')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('losses.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u0r2Gpj9WKj"
      },
      "source": [
        "# 6] Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMacvgWc2qzw"
      },
      "source": [
        "We first discretize the volumetric space at an initial resolution and evaluate the occupancy network fθ (p, x) for all p in this grid.\n",
        "We mark all grid points p as <font color=\"orange\"> occupied </font> for which fθ(p,x) is bigger or equal to some threshold τ. Next, we mark all voxels as <font color=\"olive\"> active </font> for which **at least\n",
        "two adjacent grid points** have differing occupancy predictions. These are the voxels which would intersect the mesh with the marching cubes algorithm\n",
        "at the current resolution. We subdivide **all active voxels into**<font color=\"orange\"> 8 </font>**subvoxels** and evaluate all new grid points which are introduced to the occupancy grid through\n",
        "this subdivision. We repeat these steps until the desired final resolution is reached.\n",
        "At this final resolution, we apply the Marching Cubes algorithm to extract an approximate isosurface :\n",
        "{p ∈ R3 | fθ(p,x) = τ}.\n",
        "Our algorithm converges to the correct mesh if the occupancy grid at the initial resolution contains points from every connected component of both the\n",
        "interior and the exterior of the mesh. It is hence important to take an initial resolution which is high enough to satisfy this condition.\n",
        "In practice, we found that an initial resolution of $32^3$ was sufficient in almost all cases.\n",
        "The initial mesh extracted by the Marching Cubes algorithm can be further refined. In a first step, we simplify the mesh using the\n",
        "Fast-Quadric-Mesh-Simplification algorithm. Finally, we refine the output mesh using first and second order (i.e., gradient) information.\n",
        "Towards this goal, we sample random points pk from each face of the output mesh and minimize the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y1hIAvXePXg"
      },
      "outputs": [],
      "source": [
        "def plotMesh(vertices, faces):\n",
        "    \"\"\"\n",
        "    This function is used to plot a 3D Mesh.\n",
        "\n",
        "    Args:\n",
        "        vertices (np.array): Array of mesh vertices.\n",
        "        faces (np.array): Array of mesh faces (triangles).\n",
        "\n",
        "    Returns:\n",
        "        Plot\n",
        "    \"\"\"\n",
        "    i = []\n",
        "    j = []\n",
        "    k = []\n",
        "    for face in faces:\n",
        "        i.append(face[0])\n",
        "        j.append(face[1])\n",
        "        k.append(face[2])\n",
        "    fig = go.Figure(data=[go.Mesh3d(\n",
        "        x=vertices[:, 0],\n",
        "        y=vertices[:, 1],\n",
        "        z=vertices[:, 2],\n",
        "        i=i,\n",
        "        j=j,\n",
        "        k=k,\n",
        "        opacity=0.8,\n",
        "        color='lightblue'\n",
        "    )])\n",
        "    fig.update_layout(\n",
        "        scene=dict(\n",
        "            xaxis=dict(visible=True),\n",
        "            yaxis=dict(visible=True),\n",
        "            zaxis=dict(visible=True),\n",
        "        ),\n",
        "        margin=dict(l=0, r=0, b=0, t=0)\n",
        "    )\n",
        "\n",
        "    fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9_0FERiAhMW8",
        "GFBi0FgT9WKe",
        "Rh8ugKUp9WKf",
        "8abAwINI9WKf",
        "QHoALeYs9WKg",
        "3BEqvWJV9WKh",
        "fME3ek-b9WKh",
        "_Xfwdl8S9WKi",
        "FbexQquN9WKi"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}